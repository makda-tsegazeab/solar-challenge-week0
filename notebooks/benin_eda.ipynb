{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10497fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e037ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_and_profile_data(filepath):\n",
    "    \"\"\"\n",
    "    Load dataset and perform initial profiling\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"DATA PROFILE REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nData Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Check for timestamp column\n",
    "    timestamp_cols = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]\n",
    "    if timestamp_cols:\n",
    "        print(f\"\\nTimestamp columns found: {timestamp_cols}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load your data - CALL the function with the filepath\n",
    "df = load_and_profile_data('../data/benin_data.csv')  # Adjust path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_summary(df):\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary statistics and missing value report\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Numeric columns summary\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(\"Numeric Columns Summary:\")\n",
    "    print(df[numeric_cols].describe())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MISSING VALUE REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_report = pd.DataFrame({\n",
    "        'Missing_Count': df.isnull().sum(),\n",
    "        'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "    })\n",
    "    print(missing_report)\n",
    "    \n",
    "    # Columns with >5% missing values\n",
    "    high_missing = missing_report[missing_report['Missing_Percentage'] > 5]\n",
    "    if not high_missing.empty:\n",
    "        print(f\"\\nColumns with >5% missing values:\")\n",
    "        print(high_missing)\n",
    "    else:\n",
    "        print(\"\\nNo columns with >5% missing values\")\n",
    "    \n",
    "    return numeric_cols\n",
    "\n",
    "numeric_columns = comprehensive_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d10d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    Detect outliers using Z-score method and perform basic cleaning\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"OUTLIER DETECTION REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Key columns for solar analysis\n",
    "    key_columns = ['GHI', 'DNI', 'DHI', 'ModA', 'ModB', 'WS', 'WSgust']\n",
    "    available_key_cols = [col for col in key_columns if col in df.columns]\n",
    "    \n",
    "    outlier_report = {}\n",
    "    \n",
    "    for col in available_key_cols:\n",
    "        if col in numeric_cols:\n",
    "            # Calculate Z-scores\n",
    "            z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "            outlier_count = len(z_scores[z_scores > 3])\n",
    "            outlier_percentage = (outlier_count / len(z_scores)) * 100\n",
    "            \n",
    "            outlier_report[col] = {\n",
    "                'outlier_count': outlier_count,\n",
    "                'outlier_percentage': outlier_percentage\n",
    "            }\n",
    "            \n",
    "            print(f\"{col}: {outlier_count} outliers ({outlier_percentage:.2f}%)\")\n",
    "    \n",
    "    return outlier_report, available_key_cols\n",
    "\n",
    "outlier_report, key_cols = detect_outliers(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92902b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, key_cols):\n",
    "    \"\"\"\n",
    "    Clean the dataset: handle missing values and outliers\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"DATA CLEANING PROCESS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Handle missing values in key columns (impute with median)\n",
    "    for col in key_cols:\n",
    "        if col in df_clean.columns:\n",
    "            missing_before = df_clean[col].isnull().sum()\n",
    "            if missing_before > 0:\n",
    "                df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "                print(f\"Imputed {missing_before} missing values in {col} with median\")\n",
    "    \n",
    "    # Handle outliers using IQR method (more robust than Z-score for skewed data)\n",
    "    for col in key_cols:\n",
    "        if col in df_clean.columns:\n",
    "            Q1 = df_clean[col].quantile(0.25)\n",
    "            Q3 = df_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers_before = len(df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)])\n",
    "            \n",
    "            # Cap outliers instead of removing to preserve data structure\n",
    "            df_clean[col] = np.where(df_clean[col] < lower_bound, lower_bound, df_clean[col])\n",
    "            df_clean[col] = np.where(df_clean[col] > upper_bound, upper_bound, df_clean[col])\n",
    "            \n",
    "            print(f\"Capped {outliers_before} outliers in {col}\")\n",
    "    \n",
    "    print(f\"\\nOriginal data shape: {df.shape}\")\n",
    "    print(f\"Cleaned data shape: {df_clean.shape}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_clean = clean_data(df, key_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_analysis(df_clean):\n",
    "    \"\"\"\n",
    "    Perform time series analysis on solar radiation data\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"TIME SERIES ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Identify timestamp column (you'll need to adjust this based on your data)\n",
    "    timestamp_col = None\n",
    "    for col in df_clean.columns:\n",
    "        if 'time' in col.lower() or 'date' in col.lower():\n",
    "            timestamp_col = col\n",
    "            break\n",
    "    \n",
    "    if timestamp_col:\n",
    "        # Convert to datetime if possible\n",
    "        try:\n",
    "            df_clean[timestamp_col] = pd.to_datetime(df_clean[timestamp_col])\n",
    "            df_clean = df_clean.sort_values(timestamp_col)\n",
    "            \n",
    "            # Create time-based features\n",
    "            df_clean['hour'] = df_clean[timestamp_col].dt.hour\n",
    "            df_clean['month'] = df_clean[timestamp_col].dt.month\n",
    "            df_clean['day_of_year'] = df_clean[timestamp_col].dt.dayofyear\n",
    "            \n",
    "        except:\n",
    "            print(f\"Could not convert {timestamp_col} to datetime\")\n",
    "            timestamp_col = None\n",
    "    \n",
    "    # Plot key solar metrics over time\n",
    "    solar_metrics = ['GHI', 'DNI', 'DHI', 'Tamb']\n",
    "    available_metrics = [metric for metric in solar_metrics if metric in df_clean.columns]\n",
    "    \n",
    "    if available_metrics and timestamp_col:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for i, metric in enumerate(available_metrics[:4]):\n",
    "            axes[i].plot(df_clean[timestamp_col], df_clean[metric], alpha=0.7)\n",
    "            axes[i].set_title(f'{metric} over Time')\n",
    "            axes[i].set_xlabel('Timestamp')\n",
    "            axes[i].set_ylabel(metric)\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return df_clean, timestamp_col\n",
    "\n",
    "df_clean, timestamp_col = time_series_analysis(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4207ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_impact_analysis(df_clean):\n",
    "    \"\"\"\n",
    "    Analyze the impact of cleaning events on sensor readings\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"CLEANING IMPACT ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check if cleaning flag exists\n",
    "    cleaning_flags = [col for col in df_clean.columns if 'clean' in col.lower()]\n",
    "    \n",
    "    if cleaning_flags and 'ModA' in df_clean.columns and 'ModB' in df_clean.columns:\n",
    "        cleaning_col = cleaning_flags[0]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Group by cleaning flag and plot average sensor readings\n",
    "        cleaning_groups = df_clean.groupby(cleaning_col)[['ModA', 'ModB']].mean()\n",
    "        \n",
    "        cleaning_groups.plot(kind='bar', ax=ax1)\n",
    "        ax1.set_title('Average Sensor Readings by Cleaning Event')\n",
    "        ax1.set_ylabel('Sensor Reading')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Box plot to show distribution\n",
    "        df_clean.boxplot(column=['ModA', 'ModB'], by=cleaning_col, ax=ax2)\n",
    "        ax2.set_title('Sensor Readings Distribution by Cleaning Event')\n",
    "        \n",
    "        plt.suptitle('')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistical test for significance\n",
    "        from scipy.stats import ttest_ind\n",
    "        \n",
    "        clean_events = df_clean[cleaning_col].unique()\n",
    "        if len(clean_events) == 2:\n",
    "            group1 = df_clean[df_clean[cleaning_col] == clean_events[0]]['ModA']\n",
    "            group2 = df_clean[df_clean[cleaning_col] == clean_events[1]]['ModA']\n",
    "            \n",
    "            t_stat, p_value = ttest_ind(group1, group2, nan_policy='omit')\n",
    "            print(f\"T-test for ModA difference: t-stat={t_stat:.3f}, p-value={p_value:.3f}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Cleaning flag or sensor data not available for analysis\")\n",
    "\n",
    "cleaning_impact_analysis(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f1392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(df_clean):\n",
    "    \"\"\"\n",
    "    Perform correlation analysis and relationship visualization\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Select key columns for correlation analysis\n",
    "    correlation_cols = ['GHI', 'DNI', 'DHI', 'TModA', 'TModB', 'Tamb', 'RH', 'BP', 'WS', 'WSgust']\n",
    "    available_corr_cols = [col for col in correlation_cols if col in df_clean.columns]\n",
    "    \n",
    "    if len(available_corr_cols) > 1:\n",
    "        # Correlation heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        corr_matrix = df_clean[available_corr_cols].corr()\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=0.5)\n",
    "        plt.title('Correlation Matrix of Solar Parameters')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Scatter plots for key relationships\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # WS vs GHI\n",
    "        if 'WS' in df_clean.columns and 'GHI' in df_clean.columns:\n",
    "            axes[0,0].scatter(df_clean['WS'], df_clean['GHI'], alpha=0.5)\n",
    "            axes[0,0].set_xlabel('Wind Speed (WS)')\n",
    "            axes[0,0].set_ylabel('GHI')\n",
    "            axes[0,0].set_title('Wind Speed vs GHI')\n",
    "        \n",
    "        # RH vs Tamb\n",
    "        if 'RH' in df_clean.columns and 'Tamb' in df_clean.columns:\n",
    "            axes[0,1].scatter(df_clean['RH'], df_clean['Tamb'], alpha=0.5)\n",
    "            axes[0,1].set_xlabel('Relative Humidity (RH)')\n",
    "            axes[0,1].set_ylabel('Ambient Temperature (Tamb)')\n",
    "            axes[0,1].set_title('Relative Humidity vs Ambient Temperature')\n",
    "        \n",
    "        # RH vs GHI\n",
    "        if 'RH' in df_clean.columns and 'GHI' in df_clean.columns:\n",
    "            axes[1,0].scatter(df_clean['RH'], df_clean['GHI'], alpha=0.5)\n",
    "            axes[1,0].set_xlabel('Relative Humidity (RH)')\n",
    "            axes[1,0].set_ylabel('GHI')\n",
    "            axes[1,0].set_title('Relative Humidity vs GHI')\n",
    "        \n",
    "        # WSgust vs GHI\n",
    "        if 'WSgust' in df_clean.columns and 'GHI' in df_clean.columns:\n",
    "            axes[1,1].scatter(df_clean['WSgust'], df_clean['GHI'], alpha=0.5)\n",
    "            axes[1,1].set_xlabel('Wind Gust (WSgust)')\n",
    "            axes[1,1].set_ylabel('GHI')\n",
    "            axes[1,1].set_title('Wind Gust vs GHI')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "correlation_analysis(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0069bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_analysis(df_clean):\n",
    "    \"\"\"\n",
    "    Analyze wind patterns and distributions\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"WIND ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Wind speed distribution\n",
    "    if 'WS' in df_clean.columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(df_clean['WS'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Wind Speed (WS)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Wind Speed Distribution')\n",
    "        \n",
    "        # GHI distribution\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(df_clean['GHI'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('GHI')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('GHI Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Basic wind direction analysis (if available)\n",
    "    if 'WD' in df_clean.columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(df_clean['WD'].dropna(), bins=36, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Wind Direction (degrees)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Wind Direction Distribution')\n",
    "        plt.show()\n",
    "\n",
    "wind_analysis(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_analysis(df_clean):\n",
    "    \"\"\"\n",
    "    Analyze temperature patterns and relationships with humidity\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"TEMPERATURE & HUMIDITY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Bubble chart: GHI vs Tamb with bubble size = RH\n",
    "    if all(col in df_clean.columns for col in ['GHI', 'Tamb', 'RH']):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Normalize RH for bubble sizes\n",
    "        rh_normalized = (df_clean['RH'] - df_clean['RH'].min()) / (df_clean['RH'].max() - df_clean['RH'].min())\n",
    "        bubble_sizes = rh_normalized * 100 + 10  # Scale for visibility\n",
    "        \n",
    "        scatter = plt.scatter(df_clean['Tamb'], df_clean['GHI'], \n",
    "                            s=bubble_sizes, alpha=0.6, c=df_clean['RH'], cmap='viridis')\n",
    "        \n",
    "        plt.colorbar(scatter, label='Relative Humidity (RH)')\n",
    "        plt.xlabel('Ambient Temperature (Tamb)')\n",
    "        plt.ylabel('GHI')\n",
    "        plt.title('GHI vs Ambient Temperature (Bubble size = RH)')\n",
    "        plt.show()\n",
    "    \n",
    "    # RH vs Temperature relationship\n",
    "    if all(col in df_clean.columns for col in ['RH', 'Tamb']):\n",
    "        # Calculate correlation\n",
    "        corr = df_clean['RH'].corr(df_clean['Tamb'])\n",
    "        print(f\"Correlation between RH and Tamb: {corr:.3f}\")\n",
    "        \n",
    "        # Create joint plot\n",
    "        sns.jointplot(x='RH', y='Tamb', data=df_clean, kind='hex', alpha=0.7)\n",
    "        plt.suptitle('RH vs Ambient Temperature Joint Distribution', y=1.02)\n",
    "        plt.show()\n",
    "\n",
    "temperature_analysis(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac4df284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data exported to: ../data/benin_clean.csv\n",
      "\n",
      "==================================================\n",
      "CLEANING SUMMARY REPORT\n",
      "==================================================\n",
      "Original dataset shape: (525600, 19)\n",
      "Cleaned dataset shape: (525600, 22)\n",
      "Columns processed: 22\n",
      "Total rows: 525600\n",
      "\n",
      "Data Quality After Cleaning:\n",
      "Missing values in key columns:\n",
      "  GHI: 0 missing (0.00%)\n",
      "  DNI: 0 missing (0.00%)\n",
      "  DHI: 0 missing (0.00%)\n",
      "  ModA: 0 missing (0.00%)\n",
      "  ModB: 0 missing (0.00%)\n",
      "  WS: 0 missing (0.00%)\n",
      "  WSgust: 0 missing (0.00%)\n"
     ]
    }
   ],
   "source": [
    "def export_clean_data(df_clean, country_name):\n",
    "    \"\"\"\n",
    "    Export cleaned dataset and generate summary report\n",
    "    \"\"\"\n",
    "    # Export to CSV (make sure data/ is in .gitignore)\n",
    "    output_path = f'../data/{country_name}_clean.csv'\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "    print(f\"Cleaned data exported to: {output_path}\")\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CLEANING SUMMARY REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "    print(f\"Columns processed: {len(df_clean.columns)}\")\n",
    "    print(f\"Total rows: {len(df_clean)}\")\n",
    "    \n",
    "    # Check data quality after cleaning\n",
    "    print(\"\\nData Quality After Cleaning:\")\n",
    "    print(f\"Missing values in key columns:\")\n",
    "    for col in key_cols:\n",
    "        if col in df_clean.columns:\n",
    "            missing = df_clean[col].isnull().sum()\n",
    "            print(f\"  {col}: {missing} missing ({missing/len(df_clean)*100:.2f}%)\")\n",
    "\n",
    "# Export the cleaned data\n",
    "export_clean_data(df_clean, 'benin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
